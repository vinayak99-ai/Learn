# Snowflake Agent Interaction Guide

A comprehensive guide to creating intelligent agents that interact with Snowflake data warehouses, covering connection setup, AI-driven query generation, caching strategies, and data visualization techniques.

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Prerequisites & Setup](#2-prerequisites--setup)
3. [Snowflake Connection Configuration](#3-snowflake-connection-configuration)
4. [Basic Agent Implementation](#4-basic-agent-implementation)
5. [AI-Driven Query Generation](#5-ai-driven-query-generation)
6. [Advanced Query Techniques](#6-advanced-query-techniques)
7. [Caching Strategies](#7-caching-strategies)
8. [Data Visualization Integration](#8-data-visualization-integration)
9. [Error Handling & Best Practices](#9-error-handling--best-practices)
10. [Security Considerations](#10-security-considerations)
11. [Performance Optimization](#11-performance-optimization)
12. [Complete Example Application](#12-complete-example-application)
13. [Resources & References](#13-resources--references)

---

## 1. Introduction

### What is a Snowflake Agent?

A Snowflake agent is an intelligent software component that interacts with Snowflake data warehouses to execute queries, retrieve data, and provide insights. These agents can leverage artificial intelligence to understand natural language queries, generate SQL dynamically, and present results in meaningful ways.

### Key Capabilities

- **Natural Language Processing:** Convert user questions into SQL queries
- **Schema Understanding:** Automatically discover and navigate database structures
- **Intelligent Query Optimization:** Generate efficient SQL based on schema and data patterns
- **Result Caching:** Store and reuse query results for improved performance
- **Data Visualization:** Transform query results into charts, graphs, and reports
- **Error Recovery:** Handle connection issues and query failures gracefully

### Use Cases

- **Business Intelligence Dashboards:** Real-time data analysis and reporting
- **Conversational Analytics:** Natural language interface to data warehouses
- **Automated Reporting:** Scheduled data extraction and report generation
- **Data Quality Monitoring:** Continuous validation of data integrity
- **ETL Pipeline Monitoring:** Track data pipeline performance and errors

---

## 2. Prerequisites & Setup

### Required Dependencies

```bash
# Install core dependencies
pip install snowflake-connector-python
pip install snowflake-sqlalchemy
pip install pandas
pip install python-dotenv

# Install AI/ML libraries for intelligent query generation
pip install openai
pip install langchain
pip install langchain-openai

# Install visualization libraries
pip install matplotlib
pip install plotly
pip install seaborn

# Install caching libraries
pip install redis
pip install diskcache
```

### System Requirements

- **Python:** 3.8 or higher
- **Snowflake Account:** Active Snowflake account with appropriate permissions
- **Memory:** Minimum 4GB RAM (8GB recommended for large datasets)
- **Network:** Stable internet connection for Snowflake API access

### Environment Configuration

Create a `.env` file to store sensitive credentials:

```env
# Snowflake Connection Parameters
SNOWFLAKE_ACCOUNT=your_account.region
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
SNOWFLAKE_ROLE=your_role

# OpenAI API for AI-driven query generation
OPENAI_API_KEY=your_openai_api_key

# Redis for caching (optional)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
```

---

## 3. Snowflake Connection Configuration

### Basic Connection Setup

```python
import snowflake.connector
from snowflake.connector import DictCursor
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class SnowflakeConnection:
    """
    Manages Snowflake database connections with connection pooling
    and automatic retry logic.
    """
    
    def __init__(self):
        self.connection = None
        self.cursor = None
    
    def connect(self):
        """
        Establish connection to Snowflake using environment variables.
        
        Returns:
            snowflake.connector.connection: Active Snowflake connection
        """
        try:
            self.connection = snowflake.connector.connect(
                account=os.getenv('SNOWFLAKE_ACCOUNT'),
                user=os.getenv('SNOWFLAKE_USER'),
                password=os.getenv('SNOWFLAKE_PASSWORD'),
                warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
                database=os.getenv('SNOWFLAKE_DATABASE'),
                schema=os.getenv('SNOWFLAKE_SCHEMA'),
                role=os.getenv('SNOWFLAKE_ROLE'),
                # Performance optimizations
                client_session_keep_alive=True,
                network_timeout=30,
                login_timeout=10
            )
            
            self.cursor = self.connection.cursor(DictCursor)
            print("âœ… Successfully connected to Snowflake")
            return self.connection
            
        except Exception as e:
            print(f"âŒ Error connecting to Snowflake: {str(e)}")
            raise
    
    def close(self):
        """Close cursor and connection."""
        if self.cursor:
            self.cursor.close()
        if self.connection:
            self.connection.close()
            print("ðŸ”’ Snowflake connection closed")
    
    def __enter__(self):
        """Context manager entry."""
        self.connect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()

# Usage example
if __name__ == "__main__":
    with SnowflakeConnection() as sf_conn:
        cursor = sf_conn.cursor
        cursor.execute("SELECT CURRENT_VERSION()")
        result = cursor.fetchone()
        print(f"Snowflake Version: {result['CURRENT_VERSION()']}")
```

### Advanced Connection with SQLAlchemy

```python
from sqlalchemy import create_engine
from sqlalchemy.pool import NullPool
import urllib.parse

class SnowflakeEngine:
    """
    SQLAlchemy-based Snowflake connection for ORM and
    advanced query capabilities.
    """
    
    def __init__(self):
        self.engine = None
    
    def create_engine(self):
        """
        Create SQLAlchemy engine for Snowflake.
        
        Returns:
            sqlalchemy.engine.Engine: Configured database engine
        """
        # URL encode password to handle special characters
        password = urllib.parse.quote_plus(os.getenv('SNOWFLAKE_PASSWORD'))
        
        connection_string = (
            f"snowflake://{os.getenv('SNOWFLAKE_USER')}:{password}"
            f"@{os.getenv('SNOWFLAKE_ACCOUNT')}/"
            f"{os.getenv('SNOWFLAKE_DATABASE')}/{os.getenv('SNOWFLAKE_SCHEMA')}"
            f"?warehouse={os.getenv('SNOWFLAKE_WAREHOUSE')}"
            f"&role={os.getenv('SNOWFLAKE_ROLE')}"
        )
        
        self.engine = create_engine(
            connection_string,
            poolclass=NullPool,  # Disable connection pooling
            echo=False  # Set to True for SQL debugging
        )
        
        return self.engine
    
    def execute_query(self, query, params=None):
        """
        Execute a SQL query and return results as pandas DataFrame.
        
        Args:
            query (str): SQL query to execute
            params (dict): Query parameters for prepared statements
            
        Returns:
            pandas.DataFrame: Query results
        """
        import pandas as pd
        
        try:
            with self.engine.connect() as connection:
                df = pd.read_sql(query, connection, params=params)
                return df
        except Exception as e:
            print(f"âŒ Query execution error: {str(e)}")
            raise

# Usage example
engine = SnowflakeEngine()
engine.create_engine()

query = "SELECT * FROM ORDERS LIMIT 10"
df = engine.execute_query(query)
print(df.head())
```

---

## 4. Basic Agent Implementation

### Simple Query Agent

```python
import pandas as pd
from typing import Dict, List, Any

class SnowflakeAgent:
    """
    Basic agent for executing Snowflake queries and
    processing results.
    """
    
    def __init__(self, connection):
        """
        Initialize agent with Snowflake connection.
        
        Args:
            connection: Active Snowflake connection or engine
        """
        self.connection = connection
        self.query_history = []
    
    def execute_query(self, sql: str, params: Dict = None) -> pd.DataFrame:
        """
        Execute SQL query and return results as DataFrame.
        
        Args:
            sql (str): SQL query to execute
            params (dict): Query parameters
            
        Returns:
            pd.DataFrame: Query results
        """
        try:
            # Log query
            self.query_history.append({
                'sql': sql,
                'params': params,
                'timestamp': pd.Timestamp.now()
            })
            
            # Execute query
            df = pd.read_sql(sql, self.connection, params=params)
            
            print(f"âœ… Query executed successfully. Rows returned: {len(df)}")
            return df
            
        except Exception as e:
            print(f"âŒ Query execution failed: {str(e)}")
            raise
    
    def get_schema_info(self, table_name: str = None) -> pd.DataFrame:
        """
        Retrieve schema information for tables in the current database.
        
        Args:
            table_name (str): Specific table name (optional)
            
        Returns:
            pd.DataFrame: Schema information
        """
        if table_name:
            query = f"""
            SELECT 
                TABLE_NAME,
                COLUMN_NAME,
                DATA_TYPE,
                IS_NULLABLE,
                COLUMN_DEFAULT,
                CHARACTER_MAXIMUM_LENGTH
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_NAME = '{table_name}'
            ORDER BY ORDINAL_POSITION
            """
        else:
            query = """
            SELECT 
                TABLE_NAME,
                COLUMN_NAME,
                DATA_TYPE,
                IS_NULLABLE
            FROM INFORMATION_SCHEMA.COLUMNS
            ORDER BY TABLE_NAME, ORDINAL_POSITION
            """
        
        return self.execute_query(query)
    
    def get_table_statistics(self, table_name: str) -> Dict[str, Any]:
        """
        Get statistical information about a table.
        
        Args:
            table_name (str): Name of the table
            
        Returns:
            dict: Table statistics
        """
        queries = {
            'row_count': f"SELECT COUNT(*) as count FROM {table_name}",
            'size_mb': f"""
                SELECT 
                    ROUND(BYTES / (1024 * 1024), 2) as size_mb
                FROM INFORMATION_SCHEMA.TABLES
                WHERE TABLE_NAME = '{table_name}'
            """,
            'column_count': f"""
                SELECT COUNT(*) as count
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_NAME = '{table_name}'
            """
        }
        
        stats = {}
        for key, query in queries.items():
            result = self.execute_query(query)
            stats[key] = result.iloc[0, 0] if not result.empty else None
        
        return stats

# Usage example
with SnowflakeConnection() as sf_conn:
    agent = SnowflakeAgent(sf_conn.connection)
    
    # Get schema information
    schema_info = agent.get_schema_info('CUSTOMERS')
    print(schema_info)
    
    # Get table statistics
    stats = agent.get_table_statistics('CUSTOMERS')
    print(f"Table Statistics: {stats}")
    
    # Execute custom query
    query = """
    SELECT 
        CUSTOMER_ID,
        CUSTOMER_NAME,
        TOTAL_ORDERS
    FROM CUSTOMERS
    WHERE TOTAL_ORDERS > 10
    ORDER BY TOTAL_ORDERS DESC
    LIMIT 100
    """
    results = agent.execute_query(query)
    print(results.head())
```

---

## 5. AI-Driven Query Generation

### Natural Language to SQL Conversion

```python
from openai import OpenAI
from typing import Optional
import json

class AIQueryGenerator:
    """
    Generates SQL queries from natural language using OpenAI GPT models.
    """
    
    def __init__(self, schema_context: str):
        """
        Initialize with schema context for better query generation.
        
        Args:
            schema_context (str): Description of database schema
        """
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.schema_context = schema_context
    
    def generate_sql(self, question: str, model: str = "gpt-4") -> str:
        """
        Convert natural language question to SQL query.
        
        Args:
            question (str): Natural language question
            model (str): OpenAI model to use
            
        Returns:
            str: Generated SQL query
        """
        system_prompt = f"""You are an expert SQL query generator for Snowflake.
        
Database Schema Context:
{self.schema_context}

Rules:
1. Generate valid Snowflake SQL syntax
2. Use proper table and column names from the schema
3. Include appropriate WHERE clauses, JOINs, and aggregations
4. Optimize queries for performance
5. Return ONLY the SQL query, no explanations
6. Use uppercase for SQL keywords
7. Format the query for readability
"""
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0.1,  # Low temperature for consistent output
                max_tokens=500
            )
            
            sql_query = response.choices[0].message.content.strip()
            
            # Clean up markdown code blocks if present
            if sql_query.startswith('```sql'):
                sql_query = sql_query.replace('```sql\n', '').replace('\n```', '')
            elif sql_query.startswith('```'):
                sql_query = sql_query.replace('```\n', '').replace('\n```', '')
            
            return sql_query
            
        except Exception as e:
            print(f"âŒ Error generating SQL: {str(e)}")
            raise
    
    def explain_query(self, sql_query: str) -> str:
        """
        Generate human-readable explanation of SQL query.
        
        Args:
            sql_query (str): SQL query to explain
            
        Returns:
            str: Plain English explanation
        """
        prompt = f"""Explain this SQL query in simple terms:

{sql_query}

Provide a clear, concise explanation of what this query does."""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=200
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âŒ Error explaining query: {str(e)}")
            return "Unable to generate explanation"

# Usage example
schema_context = """
Tables:
1. CUSTOMERS (customer_id, customer_name, email, signup_date, country)
2. ORDERS (order_id, customer_id, order_date, total_amount, status)
3. PRODUCTS (product_id, product_name, category, price, stock_quantity)
4. ORDER_ITEMS (order_item_id, order_id, product_id, quantity, unit_price)
"""

ai_generator = AIQueryGenerator(schema_context)

# Generate SQL from natural language
question = "Show me the top 10 customers by total order value in 2024"
sql_query = ai_generator.generate_sql(question)
print(f"Generated SQL:\n{sql_query}\n")

# Get explanation
explanation = ai_generator.explain_query(sql_query)
print(f"Explanation: {explanation}")
```

### LangChain Integration for Advanced AI Agents

```python
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain_openai import ChatOpenAI
from langchain.agents.agent_types import AgentType

class LangChainSnowflakeAgent:
    """
    Advanced agent using LangChain for multi-step reasoning
    and query generation.
    """
    
    def __init__(self, engine):
        """
        Initialize LangChain agent with Snowflake engine.
        
        Args:
            engine: SQLAlchemy engine for Snowflake
        """
        # Create SQLDatabase from engine
        self.db = SQLDatabase(engine)
        
        # Initialize ChatOpenAI
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Create SQL toolkit
        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)
        
        # Create agent
        self.agent = create_sql_agent(
            llm=self.llm,
            toolkit=toolkit,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            max_iterations=10,
            max_execution_time=60,
            early_stopping_method="generate"
        )
    
    def query(self, question: str) -> str:
        """
        Execute natural language query using LangChain agent.
        
        Args:
            question (str): Natural language question
            
        Returns:
            str: Agent's response
        """
        try:
            response = self.agent.run(question)
            return response
        except Exception as e:
            print(f"âŒ Agent query failed: {str(e)}")
            return f"Error: {str(e)}"
    
    def get_relevant_tables(self, question: str) -> List[str]:
        """
        Identify relevant tables for a given question.
        
        Args:
            question (str): Natural language question
            
        Returns:
            list: Relevant table names
        """
        prompt = f"""Given this question: '{question}'
        
        And these available tables: {self.db.get_usable_table_names()}
        
        Which tables are most relevant? Return as comma-separated list."""
        
        response = self.llm.predict(prompt)
        tables = [t.strip() for t in response.split(',')]
        return tables

# Usage example
engine = SnowflakeEngine().create_engine()
lc_agent = LangChainSnowflakeAgent(engine)

# Ask complex questions
questions = [
    "What are the top 5 products by revenue in Q4 2024?",
    "Which customers have made more than 10 orders but haven't ordered in the last 30 days?",
    "Show me the average order value by country for the last quarter"
]

for question in questions:
    print(f"\n{'='*60}")
    print(f"Question: {question}")
    print(f"{'='*60}")
    answer = lc_agent.query(question)
    print(f"Answer: {answer}")
```

---

## 6. Advanced Query Techniques

### Query Optimization and Parameterization

```python
class OptimizedQueryBuilder:
    """
    Builds optimized SQL queries with proper parameterization
    and performance hints.
    """
    
    @staticmethod
    def build_parameterized_query(
        table: str,
        columns: List[str],
        filters: Dict[str, Any] = None,
        order_by: List[str] = None,
        limit: int = None
    ) -> tuple:
        """
        Build parameterized SQL query to prevent SQL injection.
        
        Args:
            table (str): Table name
            columns (list): Column names to select
            filters (dict): Filter conditions
            order_by (list): Columns to order by
            limit (int): Result limit
            
        Returns:
            tuple: (query_string, parameters_dict)
        """
        # Build SELECT clause
        select_clause = ", ".join(columns)
        query = f"SELECT {select_clause} FROM {table}"
        params = {}
        
        # Build WHERE clause
        if filters:
            conditions = []
            for i, (col, value) in enumerate(filters.items()):
                param_name = f"param_{i}"
                conditions.append(f"{col} = :{param_name}")
                params[param_name] = value
            query += " WHERE " + " AND ".join(conditions)
        
        # Build ORDER BY clause
        if order_by:
            query += " ORDER BY " + ", ".join(order_by)
        
        # Build LIMIT clause
        if limit:
            query += f" LIMIT {limit}"
        
        return query, params
    
    @staticmethod
    def build_aggregation_query(
        table: str,
        group_by: List[str],
        aggregations: Dict[str, str],
        filters: Dict[str, Any] = None,
        having: str = None
    ) -> str:
        """
        Build aggregation query with GROUP BY.
        
        Args:
            table (str): Table name
            group_by (list): Columns to group by
            aggregations (dict): {alias: aggregation_expression}
            filters (dict): WHERE clause filters
            having (str): HAVING clause condition
            
        Returns:
            str: SQL query
        """
        # Build SELECT clause
        select_items = group_by.copy()
        for alias, expr in aggregations.items():
            select_items.append(f"{expr} AS {alias}")
        
        query = f"SELECT {', '.join(select_items)} FROM {table}"
        
        # Add WHERE clause
        if filters:
            conditions = [f"{k} = '{v}'" for k, v in filters.items()]
            query += " WHERE " + " AND ".join(conditions)
        
        # Add GROUP BY
        query += f" GROUP BY {', '.join(group_by)}"
        
        # Add HAVING
        if having:
            query += f" HAVING {having}"
        
        return query

# Usage examples
builder = OptimizedQueryBuilder()

# Parameterized query
query, params = builder.build_parameterized_query(
    table="CUSTOMERS",
    columns=["customer_id", "customer_name", "email"],
    filters={"country": "USA", "status": "active"},
    order_by=["customer_name"],
    limit=100
)
print(f"Query: {query}")
print(f"Params: {params}")

# Aggregation query
agg_query = builder.build_aggregation_query(
    table="ORDERS",
    group_by=["CUSTOMER_ID", "ORDER_STATUS"],
    aggregations={
        "total_orders": "COUNT(*)",
        "total_revenue": "SUM(total_amount)",
        "avg_order_value": "AVG(total_amount)"
    },
    filters={"order_date": "2024"},
    having="COUNT(*) > 5"
)
print(f"Aggregation Query:\n{agg_query}")
```

### Dynamic Schema Discovery

```python
class SchemaDiscovery:
    """
    Dynamically discovers and analyzes Snowflake database schema.
    """
    
    def __init__(self, connection):
        self.connection = connection
    
    def get_all_tables(self) -> pd.DataFrame:
        """Get list of all tables in current schema."""
        query = """
        SELECT 
            TABLE_CATALOG,
            TABLE_SCHEMA,
            TABLE_NAME,
            TABLE_TYPE,
            ROW_COUNT,
            BYTES,
            CREATED,
            LAST_ALTERED
        FROM INFORMATION_SCHEMA.TABLES
        WHERE TABLE_SCHEMA = CURRENT_SCHEMA()
        ORDER BY TABLE_NAME
        """
        return pd.read_sql(query, self.connection)
    
    def get_table_columns(self, table_name: str) -> pd.DataFrame:
        """Get detailed column information for a table."""
        query = f"""
        SELECT 
            COLUMN_NAME,
            ORDINAL_POSITION,
            IS_NULLABLE,
            DATA_TYPE,
            CHARACTER_MAXIMUM_LENGTH,
            NUMERIC_PRECISION,
            NUMERIC_SCALE,
            COLUMN_DEFAULT,
            IS_IDENTITY
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_NAME = '{table_name}'
        AND TABLE_SCHEMA = CURRENT_SCHEMA()
        ORDER BY ORDINAL_POSITION
        """
        return pd.read_sql(query, self.connection)
    
    def get_foreign_keys(self, table_name: str) -> pd.DataFrame:
        """Get foreign key relationships for a table."""
        query = f"""
        SELECT
            FK.CONSTRAINT_NAME,
            FK.TABLE_NAME AS FK_TABLE,
            KCU1.COLUMN_NAME AS FK_COLUMN,
            PK.TABLE_NAME AS PK_TABLE,
            KCU2.COLUMN_NAME AS PK_COLUMN
        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS FK
        INNER JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU1
            ON FK.CONSTRAINT_NAME = KCU1.CONSTRAINT_NAME
        INNER JOIN INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS RC
            ON FK.CONSTRAINT_NAME = RC.CONSTRAINT_NAME
        INNER JOIN INFORMATION_SCHEMA.TABLE_CONSTRAINTS PK
            ON RC.UNIQUE_CONSTRAINT_NAME = PK.CONSTRAINT_NAME
        INNER JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE KCU2
            ON PK.CONSTRAINT_NAME = KCU2.CONSTRAINT_NAME
        WHERE FK.CONSTRAINT_TYPE = 'FOREIGN KEY'
        AND FK.TABLE_NAME = '{table_name}'
        """
        return pd.read_sql(query, self.connection)
    
    def generate_schema_context(self) -> str:
        """
        Generate comprehensive schema context for AI query generation.
        
        Returns:
            str: Formatted schema description
        """
        tables = self.get_all_tables()
        context_parts = ["Database Schema:\n"]
        
        for _, table in tables.iterrows():
            table_name = table['TABLE_NAME']
            columns = self.get_table_columns(table_name)
            
            context_parts.append(f"\nTable: {table_name}")
            context_parts.append(f"Rows: {table['ROW_COUNT']}")
            context_parts.append("Columns:")
            
            for _, col in columns.iterrows():
                col_info = f"  - {col['COLUMN_NAME']} ({col['DATA_TYPE']})"
                if col['IS_NULLABLE'] == 'NO':
                    col_info += " NOT NULL"
                context_parts.append(col_info)
        
        return "\n".join(context_parts)

# Usage example
with SnowflakeConnection() as sf_conn:
    discovery = SchemaDiscovery(sf_conn.connection)
    
    # Get all tables
    tables = discovery.get_all_tables()
    print(f"Found {len(tables)} tables")
    
    # Analyze specific table
    columns = discovery.get_table_columns('CUSTOMERS')
    print(f"\nCUSTOMERS table columns:\n{columns}")
    
    # Generate schema context for AI
    schema_context = discovery.generate_schema_context()
    print(f"\nSchema Context:\n{schema_context}")
```


---

## 7. Caching Strategies

### In-Memory Caching with Python

```python
from functools import lru_cache
from datetime import datetime, timedelta
import hashlib
import pickle

class QueryCache:
    """
    Implements caching layer for Snowflake query results
    to reduce redundant database calls.
    """
    
    def __init__(self, max_size: int = 100, ttl_minutes: int = 60):
        """
        Initialize cache with size and TTL limits.
        
        Args:
            max_size (int): Maximum number of cached queries
            ttl_minutes (int): Time-to-live in minutes
        """
        self.cache = {}
        self.max_size = max_size
        self.ttl = timedelta(minutes=ttl_minutes)
    
    def _generate_key(self, query: str, params: dict = None) -> str:
        """Generate unique cache key from query and parameters."""
        key_str = query + str(params) if params else query
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, query: str, params: dict = None) -> Optional[pd.DataFrame]:
        """
        Retrieve cached query result if available and not expired.
        
        Args:
            query (str): SQL query
            params (dict): Query parameters
            
        Returns:
            pd.DataFrame or None: Cached result if available
        """
        key = self._generate_key(query, params)
        
        if key in self.cache:
            cached_data, timestamp = self.cache[key]
            
            # Check if cache entry is still valid
            if datetime.now() - timestamp < self.ttl:
                print(f"âœ… Cache hit for query")
                return cached_data
            else:
                # Remove expired entry
                del self.cache[key]
                print(f"â±ï¸ Cache expired for query")
        
        print(f"âŒ Cache miss for query")
        return None
    
    def set(self, query: str, result: pd.DataFrame, params: dict = None):
        """
        Store query result in cache.
        
        Args:
            query (str): SQL query
            result (pd.DataFrame): Query result to cache
            params (dict): Query parameters
        """
        # Implement LRU eviction if cache is full
        if len(self.cache) >= self.max_size:
            # Remove oldest entry
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]
            print(f"ðŸ—‘ï¸ Evicted oldest cache entry")
        
        key = self._generate_key(query, params)
        self.cache[key] = (result.copy(), datetime.now())
        print(f"ðŸ’¾ Cached query result ({len(result)} rows)")
    
    def clear(self):
        """Clear all cached entries."""
        self.cache.clear()
        print(f"ðŸ§¹ Cache cleared")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'ttl_minutes': self.ttl.total_seconds() / 60
        }

class CachedSnowflakeAgent(SnowflakeAgent):
    """
    Extended agent with query result caching.
    """
    
    def __init__(self, connection, cache_ttl_minutes: int = 60):
        super().__init__(connection)
        self.cache = QueryCache(ttl_minutes=cache_ttl_minutes)
    
    def execute_query(self, sql: str, params: Dict = None, 
                     use_cache: bool = True) -> pd.DataFrame:
        """
        Execute query with caching support.
        
        Args:
            sql (str): SQL query
            params (dict): Query parameters
            use_cache (bool): Whether to use cache
            
        Returns:
            pd.DataFrame: Query results
        """
        # Try to get from cache
        if use_cache:
            cached_result = self.cache.get(sql, params)
            if cached_result is not None:
                return cached_result
        
        # Execute query
        result = super().execute_query(sql, params)
        
        # Store in cache
        if use_cache:
            self.cache.set(sql, result, params)
        
        return result

# Usage example
with SnowflakeConnection() as sf_conn:
    agent = CachedSnowflakeAgent(sf_conn.connection, cache_ttl_minutes=30)
    
    query = "SELECT * FROM ORDERS WHERE order_date >= '2024-01-01'"
    
    # First execution - cache miss
    result1 = agent.execute_query(query)
    
    # Second execution - cache hit
    result2 = agent.execute_query(query)
    
    # Check cache stats
    stats = agent.cache.get_stats()
    print(f"Cache stats: {stats}")
```

### Redis-Based Distributed Caching

```python
import redis
import json

class RedisQueryCache:
    """
    Distributed caching using Redis for multi-instance deployments.
    """
    
    def __init__(self, host: str = 'localhost', port: int = 6379, 
                 db: int = 0, ttl_seconds: int = 3600):
        """
        Initialize Redis cache connection.
        
        Args:
            host (str): Redis host
            port (int): Redis port
            db (int): Redis database number
            ttl_seconds (int): Cache expiration time
        """
        self.redis_client = redis.Redis(
            host=host,
            port=port,
            db=db,
            decode_responses=False
        )
        self.ttl = ttl_seconds
    
    def _generate_key(self, query: str, params: dict = None) -> str:
        """Generate Redis key from query."""
        key_str = f"snowflake:query:{query}"
        if params:
            key_str += f":{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, query: str, params: dict = None) -> Optional[pd.DataFrame]:
        """
        Retrieve cached query result from Redis.
        
        Args:
            query (str): SQL query
            params (dict): Query parameters
            
        Returns:
            pd.DataFrame or None: Cached result if available
        """
        key = self._generate_key(query, params)
        
        try:
            cached_data = self.redis_client.get(key)
            if cached_data:
                # Deserialize DataFrame from pickle
                df = pickle.loads(cached_data)
                print(f"âœ… Redis cache hit")
                return df
        except Exception as e:
            print(f"âš ï¸ Redis cache error: {str(e)}")
        
        print(f"âŒ Redis cache miss")
        return None
    
    def set(self, query: str, result: pd.DataFrame, params: dict = None):
        """
        Store query result in Redis.
        
        Args:
            query (str): SQL query
            result (pd.DataFrame): Query result
            params (dict): Query parameters
        """
        key = self._generate_key(query, params)
        
        try:
            # Serialize DataFrame with pickle
            serialized_data = pickle.dumps(result)
            self.redis_client.setex(key, self.ttl, serialized_data)
            print(f"ðŸ’¾ Cached to Redis (TTL: {self.ttl}s)")
        except Exception as e:
            print(f"âš ï¸ Redis cache error: {str(e)}")
    
    def clear_pattern(self, pattern: str = "snowflake:query:*"):
        """
        Clear cache entries matching pattern.
        
        Args:
            pattern (str): Redis key pattern
        """
        keys = self.redis_client.keys(pattern)
        if keys:
            self.redis_client.delete(*keys)
            print(f"ðŸ§¹ Cleared {len(keys)} cache entries")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get Redis cache statistics."""
        info = self.redis_client.info('stats')
        return {
            'hits': info.get('keyspace_hits', 0),
            'misses': info.get('keyspace_misses', 0),
            'keys': self.redis_client.dbsize()
        }

# Usage example
redis_cache = RedisQueryCache(
    host=os.getenv('REDIS_HOST', 'localhost'),
    ttl_seconds=1800  # 30 minutes
)

class RedisSnowflakeAgent(SnowflakeAgent):
    """Agent with Redis caching."""
    
    def __init__(self, connection, redis_cache):
        super().__init__(connection)
        self.cache = redis_cache
    
    def execute_query(self, sql: str, params: Dict = None,
                     use_cache: bool = True) -> pd.DataFrame:
        """Execute query with Redis caching."""
        if use_cache:
            cached = self.cache.get(sql, params)
            if cached is not None:
                return cached
        
        result = super().execute_query(sql, params)
        
        if use_cache:
            self.cache.set(sql, result, params)
        
        return result

# Usage
with SnowflakeConnection() as sf_conn:
    agent = RedisSnowflakeAgent(sf_conn.connection, redis_cache)
    
    query = "SELECT * FROM CUSTOMERS WHERE country = 'USA'"
    result = agent.execute_query(query)
    
    # Check cache stats
    stats = redis_cache.get_stats()
    print(f"Redis Stats: {stats}")
```

---

## 8. Data Visualization Integration

### Matplotlib Visualizations

```python
import matplotlib.pyplot as plt
import seaborn as sns

class DataVisualizer:
    """
    Creates visualizations from Snowflake query results.
    """
    
    def __init__(self, style: str = 'seaborn-v0_8'):
        """
        Initialize visualizer with matplotlib style.
        
        Args:
            style (str): Matplotlib style to use
        """
        plt.style.use(style)
        sns.set_palette("husl")
    
    def plot_bar_chart(self, df: pd.DataFrame, x_col: str, y_col: str,
                       title: str = None, save_path: str = None):
        """
        Create bar chart from DataFrame.
        
        Args:
            df (pd.DataFrame): Data to plot
            x_col (str): Column for x-axis
            y_col (str): Column for y-axis
            title (str): Chart title
            save_path (str): Path to save figure
        """
        plt.figure(figsize=(12, 6))
        plt.bar(df[x_col], df[y_col], color='steelblue', alpha=0.8)
        
        plt.xlabel(x_col, fontsize=12)
        plt.ylabel(y_col, fontsize=12)
        plt.title(title or f'{y_col} by {x_col}', fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ“Š Chart saved to {save_path}")
        
        plt.show()
    
    def plot_line_chart(self, df: pd.DataFrame, x_col: str, y_cols: List[str],
                        title: str = None, save_path: str = None):
        """
        Create line chart with multiple series.
        
        Args:
            df (pd.DataFrame): Data to plot
            x_col (str): Column for x-axis
            y_cols (list): Columns for y-axis (can be multiple)
            title (str): Chart title
            save_path (str): Path to save figure
        """
        plt.figure(figsize=(14, 7))
        
        for y_col in y_cols:
            plt.plot(df[x_col], df[y_col], marker='o', 
                    linewidth=2, label=y_col)
        
        plt.xlabel(x_col, fontsize=12)
        plt.ylabel('Value', fontsize=12)
        plt.title(title or f'Trends over {x_col}', fontsize=14, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(alpha=0.3)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ“ˆ Chart saved to {save_path}")
        
        plt.show()
    
    def plot_heatmap(self, df: pd.DataFrame, title: str = None,
                     save_path: str = None):
        """
        Create correlation heatmap.
        
        Args:
            df (pd.DataFrame): Data to plot (numeric columns)
            title (str): Chart title
            save_path (str): Path to save figure
        """
        plt.figure(figsize=(10, 8))
        
        # Calculate correlation matrix
        corr_matrix = df.select_dtypes(include=['number']).corr()
        
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', 
                   cmap='coolwarm', center=0, square=True,
                   linewidths=1, cbar_kws={"shrink": 0.8})
        
        plt.title(title or 'Correlation Heatmap', 
                 fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ðŸ”¥ Heatmap saved to {save_path}")
        
        plt.show()

# Usage example
with SnowflakeConnection() as sf_conn:
    agent = SnowflakeAgent(sf_conn.connection)
    
    # Query monthly sales data
    query = """
    SELECT 
        DATE_TRUNC('month', order_date) as month,
        COUNT(*) as order_count,
        SUM(total_amount) as total_revenue,
        AVG(total_amount) as avg_order_value
    FROM ORDERS
    WHERE order_date >= '2024-01-01'
    GROUP BY month
    ORDER BY month
    """
    
    df = agent.execute_query(query)
    
    # Create visualizations
    viz = DataVisualizer()
    
    viz.plot_line_chart(
        df, 
        x_col='MONTH',
        y_cols=['ORDER_COUNT', 'TOTAL_REVENUE'],
        title='Monthly Sales Trends 2024',
        save_path='/tmp/monthly_sales.png'
    )
    
    viz.plot_bar_chart(
        df,
        x_col='MONTH',
        y_col='AVG_ORDER_VALUE',
        title='Average Order Value by Month',
        save_path='/tmp/avg_order_value.png'
    )
```

### Interactive Plotly Dashboards

```python
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class InteractiveDashboard:
    """
    Creates interactive dashboards using Plotly.
    """
    
    def create_sales_dashboard(self, df: pd.DataFrame) -> go.Figure:
        """
        Create comprehensive sales dashboard.
        
        Args:
            df (pd.DataFrame): Sales data
            
        Returns:
            go.Figure: Plotly figure object
        """
        # Create subplots
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'Revenue Trend',
                'Top Products',
                'Regional Distribution',
                'Order Status'
            ),
            specs=[
                [{"type": "scatter"}, {"type": "bar"}],
                [{"type": "pie"}, {"type": "bar"}]
            ]
        )
        
        # Revenue trend (assuming df has 'date' and 'revenue' columns)
        fig.add_trace(
            go.Scatter(
                x=df['date'],
                y=df['revenue'],
                mode='lines+markers',
                name='Revenue',
                line=dict(color='#1f77b4', width=3)
            ),
            row=1, col=1
        )
        
        # Top products
        top_products = df.groupby('product')['revenue'].sum().nlargest(10)
        fig.add_trace(
            go.Bar(
                x=top_products.values,
                y=top_products.index,
                orientation='h',
                name='Products',
                marker_color='#ff7f0e'
            ),
            row=1, col=2
        )
        
        # Regional distribution
        regional_data = df.groupby('region')['revenue'].sum()
        fig.add_trace(
            go.Pie(
                labels=regional_data.index,
                values=regional_data.values,
                name='Regions'
            ),
            row=2, col=1
        )
        
        # Order status
        status_data = df.groupby('status')['order_id'].count()
        fig.add_trace(
            go.Bar(
                x=status_data.index,
                y=status_data.values,
                name='Status',
                marker_color='#2ca02c'
            ),
            row=2, col=2
        )
        
        # Update layout
        fig.update_layout(
            title_text="Sales Performance Dashboard",
            showlegend=False,
            height=800,
            width=1200
        )
        
        return fig
    
    def create_time_series(self, df: pd.DataFrame, date_col: str,
                          value_cols: List[str]) -> go.Figure:
        """
        Create interactive time series chart.
        
        Args:
            df (pd.DataFrame): Time series data
            date_col (str): Date column name
            value_cols (list): Value columns to plot
            
        Returns:
            go.Figure: Plotly figure
        """
        fig = go.Figure()
        
        for col in value_cols:
            fig.add_trace(go.Scatter(
                x=df[date_col],
                y=df[col],
                mode='lines+markers',
                name=col,
                hovertemplate=f'<b>{col}</b><br>Date: %{{x}}<br>Value: %{{y:,.0f}}<extra></extra>'
            ))
        
        fig.update_layout(
            title='Time Series Analysis',
            xaxis_title='Date',
            yaxis_title='Value',
            hovermode='x unified',
            template='plotly_white',
            height=600,
            width=1000
        )
        
        # Add range slider
        fig.update_xaxes(rangeslider_visible=True)
        
        return fig
    
    def save_html(self, fig: go.Figure, filename: str):
        """Save interactive figure as HTML."""
        fig.write_html(filename)
        print(f"ðŸ“Š Interactive dashboard saved to {filename}")

# Usage example
with SnowflakeConnection() as sf_conn:
    agent = SnowflakeAgent(sf_conn.connection)
    
    query = """
    SELECT 
        order_date as date,
        product_name as product,
        region,
        order_status as status,
        order_id,
        total_amount as revenue
    FROM ORDERS_VIEW
    WHERE order_date >= '2024-01-01'
    """
    
    df = agent.execute_query(query)
    
    # Create interactive dashboard
    dashboard = InteractiveDashboard()
    fig = dashboard.create_sales_dashboard(df)
    dashboard.save_html(fig, '/tmp/sales_dashboard.html')
    
    # Create time series
    ts_query = """
    SELECT 
        DATE_TRUNC('day', order_date) as date,
        SUM(total_amount) as revenue,
        COUNT(*) as orders
    FROM ORDERS
    WHERE order_date >= '2024-01-01'
    GROUP BY date
    ORDER BY date
    """
    
    ts_df = agent.execute_query(ts_query)
    ts_fig = dashboard.create_time_series(ts_df, 'DATE', ['REVENUE', 'ORDERS'])
    dashboard.save_html(ts_fig, '/tmp/time_series.html')
```


---

## 9. Error Handling & Best Practices

### Robust Error Handling

```python
from typing import Optional, Callable
import time
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SnowflakeErrorHandler:
    """
    Implements comprehensive error handling and retry logic
    for Snowflake operations.
    """
    
    @staticmethod
    def retry_on_failure(
        func: Callable,
        max_retries: int = 3,
        delay: int = 5,
        backoff: int = 2
    ) -> Callable:
        """
        Decorator for automatic retry with exponential backoff.
        
        Args:
            func (Callable): Function to retry
            max_retries (int): Maximum number of retry attempts
            delay (int): Initial delay between retries (seconds)
            backoff (int): Backoff multiplier
            
        Returns:
            Callable: Decorated function
        """
        def wrapper(*args, **kwargs):
            current_delay = delay
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt < max_retries:
                        logger.warning(
                            f"Attempt {attempt + 1} failed: {str(e)}. "
                            f"Retrying in {current_delay} seconds..."
                        )
                        time.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logger.error(
                            f"All {max_retries + 1} attempts failed. "
                            f"Final error: {str(e)}"
                        )
            
            raise last_exception
        
        return wrapper
    
    @staticmethod
    def handle_snowflake_errors(func: Callable) -> Callable:
        """
        Decorator for Snowflake-specific error handling.
        
        Args:
            func (Callable): Function to wrap
            
        Returns:
            Callable: Decorated function
        """
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except snowflake.connector.errors.ProgrammingError as e:
                logger.error(f"SQL Programming Error: {str(e)}")
                # Handle SQL syntax errors, invalid table/column names
                raise ValueError(f"Invalid SQL query: {str(e)}")
            except snowflake.connector.errors.DatabaseError as e:
                logger.error(f"Database Error: {str(e)}")
                # Handle connection issues, timeout, etc.
                raise ConnectionError(f"Database connection failed: {str(e)}")
            except snowflake.connector.errors.DataError as e:
                logger.error(f"Data Error: {str(e)}")
                # Handle data type mismatches, constraint violations
                raise ValueError(f"Data validation error: {str(e)}")
            except Exception as e:
                logger.error(f"Unexpected error: {str(e)}")
                raise
        
        return wrapper

class RobustSnowflakeAgent(SnowflakeAgent):
    """
    Agent with comprehensive error handling and recovery.
    """
    
    def __init__(self, connection, max_retries: int = 3):
        super().__init__(connection)
        self.max_retries = max_retries
        self.error_handler = SnowflakeErrorHandler()
    
    @SnowflakeErrorHandler.retry_on_failure(max_retries=3, delay=5)
    @SnowflakeErrorHandler.handle_snowflake_errors
    def execute_query(self, sql: str, params: Dict = None) -> pd.DataFrame:
        """
        Execute query with automatic retry and error handling.
        
        Args:
            sql (str): SQL query
            params (dict): Query parameters
            
        Returns:
            pd.DataFrame: Query results
        """
        # Validate query before execution
        if not sql or not sql.strip():
            raise ValueError("SQL query cannot be empty")
        
        # Log query execution
        logger.info(f"Executing query: {sql[:100]}...")
        
        try:
            result = super().execute_query(sql, params)
            logger.info(f"Query successful. Rows returned: {len(result)}")
            return result
        except Exception as e:
            logger.error(f"Query failed: {str(e)}")
            raise
    
    def execute_with_timeout(self, sql: str, timeout_seconds: int = 300) -> pd.DataFrame:
        """
        Execute query with timeout protection (cross-platform).
        
        Args:
            sql (str): SQL query
            timeout_seconds (int): Maximum execution time
            
        Returns:
            pd.DataFrame: Query results
        """
        import threading
        
        result_container = {'result': None, 'error': None}
        
        def execute_query_thread():
            try:
                result_container['result'] = self.execute_query(sql)
            except Exception as e:
                result_container['error'] = e
        
        # Start query execution in a separate thread
        thread = threading.Thread(target=execute_query_thread)
        thread.daemon = True
        thread.start()
        
        # Wait for completion with timeout
        thread.join(timeout=timeout_seconds)
        
        if thread.is_alive():
            logger.error(f"Query timed out after {timeout_seconds} seconds")
            raise TimeoutError(f"Query exceeded {timeout_seconds} seconds")
        
        if result_container['error']:
            raise result_container['error']
        
        return result_container['result']

# Usage example
with SnowflakeConnection() as sf_conn:
    agent = RobustSnowflakeAgent(sf_conn.connection)
    
    try:
        # This will automatically retry on failure
        result = agent.execute_query(
            "SELECT * FROM CUSTOMERS WHERE country = %(country)s",
            params={'country': 'USA'}
        )
        print(f"Query successful: {len(result)} rows")
    except Exception as e:
        print(f"Query failed after retries: {str(e)}")
```

### Best Practices Summary

```python
class SnowflakeBestPractices:
    """
    Collection of best practices for Snowflake agent development.
    """
    
    @staticmethod
    def optimize_query_performance():
        """
        Guidelines for query optimization.
        """
        best_practices = """
        1. **Use WHERE clauses to filter data early**
           - Reduce data scanned by applying filters as early as possible
           - Use partition pruning when working with clustered tables
        
        2. **Leverage clustering keys**
           - Cluster tables on frequently filtered columns
           - Monitor clustering depth and re-cluster when necessary
        
        3. **Use appropriate data types**
           - Choose the smallest data type that fits your data
           - Avoid VARCHAR(MAX) when a smaller size is sufficient
        
        4. **Limit result sets**
           - Always use LIMIT for exploratory queries
           - Paginate large result sets instead of fetching all at once
        
        5. **Use result caching**
           - Enable query result caching for repeated queries
           - Cache query results are retained for 24 hours
        
        6. **Optimize JOINs**
           - Put the largest table first in JOIN operations
           - Use INNER JOINs instead of OUTER JOINs when possible
           - Consider using CTEs for complex queries
        
        7. **Use warehouse sizing appropriately**
           - Start with smaller warehouses and scale up if needed
           - Use auto-suspend and auto-resume to optimize costs
           - Consider multi-cluster warehouses for concurrent workloads
        
        8. **Monitor credit consumption**
           - Track warehouse usage and optimize query patterns
           - Use resource monitors to set credit limits
           - Review query history to identify expensive queries
        """
        return best_practices
    
    @staticmethod
    def security_guidelines():
        """
        Security best practices for Snowflake agents.
        """
        guidelines = """
        1. **Never hardcode credentials**
           - Use environment variables or secret management systems
           - Rotate credentials regularly
           - Use key pair authentication for enhanced security
        
        2. **Implement least privilege access**
           - Grant minimum necessary permissions to service accounts
           - Use separate roles for different operations (read/write)
           - Regularly audit role assignments
        
        3. **Use parameterized queries**
           - Always use query parameters to prevent SQL injection
           - Never concatenate user input directly into SQL strings
           - Validate and sanitize all user inputs
        
        4. **Enable network policies**
           - Restrict access to specific IP addresses
           - Use private connectivity options (PrivateLink, AWS PrivateLink)
           - Enable MFA for all user accounts
        
        5. **Monitor and audit access**
           - Enable query history and access logs
           - Monitor for suspicious query patterns
           - Set up alerts for unauthorized access attempts
        
        6. **Encrypt sensitive data**
           - Enable encryption at rest and in transit
           - Use Snowflake's encryption features for sensitive columns
           - Implement data masking policies for PII
        
        7. **Manage session security**
           - Set appropriate session timeout values
           - Use client_session_keep_alive cautiously
           - Implement proper connection pooling
        """
        return guidelines
    
    @staticmethod
    def data_quality_checks():
        """
        Data quality validation patterns.
        """
        return {
            'null_check': "SELECT COUNT(*) FROM table WHERE column IS NULL",
            'duplicate_check': "SELECT column, COUNT(*) as cnt FROM table GROUP BY column HAVING cnt > 1",
            'range_check': "SELECT * FROM table WHERE numeric_column < 0 OR numeric_column > 1000",
            'format_check': "SELECT * FROM table WHERE email NOT LIKE '%@%.%'",
            'referential_integrity': """
                SELECT t1.* FROM table1 t1
                LEFT JOIN table2 t2 ON t1.fk = t2.pk
                WHERE t2.pk IS NULL
            """
        }

# Usage example
bp = SnowflakeBestPractices()
print(bp.optimize_query_performance())
print(bp.security_guidelines())

# Run data quality checks
with SnowflakeConnection() as sf_conn:
    agent = SnowflakeAgent(sf_conn.connection)
    
    checks = bp.data_quality_checks()
    for check_name, query in checks.items():
        try:
            result = agent.execute_query(query)
            print(f"âœ… {check_name}: {len(result)} issues found")
        except Exception as e:
            print(f"âŒ {check_name} failed: {str(e)}")
```

---

## 10. Security Considerations

### Secure Credential Management

```python
import keyring
from cryptography.fernet import Fernet
import base64

class SecureCredentialManager:
    """
    Manages Snowflake credentials securely using encryption
    and system keyring.
    """
    
    def __init__(self, service_name: str = 'snowflake_agent'):
        """
        Initialize credential manager.
        
        Args:
            service_name (str): Service identifier for keyring
        """
        self.service_name = service_name
        self.encryption_key = self._get_or_create_key()
        self.cipher = Fernet(self.encryption_key)
    
    def _get_or_create_key(self) -> bytes:
        """Get or create encryption key from system keyring."""
        key = keyring.get_password(self.service_name, 'encryption_key')
        
        if not key:
            # Generate new key
            key = Fernet.generate_key().decode()
            keyring.set_password(self.service_name, 'encryption_key', key)
        
        return key.encode() if isinstance(key, str) else key
    
    def store_credential(self, key: str, value: str):
        """
        Store encrypted credential.
        
        Args:
            key (str): Credential identifier
            value (str): Credential value to encrypt
        """
        encrypted_value = self.cipher.encrypt(value.encode()).decode()
        keyring.set_password(self.service_name, key, encrypted_value)
        logger.info(f"âœ… Credential '{key}' stored securely")
    
    def retrieve_credential(self, key: str) -> Optional[str]:
        """
        Retrieve and decrypt credential.
        
        Args:
            key (str): Credential identifier
            
        Returns:
            str: Decrypted credential value
        """
        encrypted_value = keyring.get_password(self.service_name, key)
        
        if not encrypted_value:
            logger.warning(f"âš ï¸ Credential '{key}' not found")
            return None
        
        decrypted_value = self.cipher.decrypt(encrypted_value.encode()).decode()
        return decrypted_value
    
    def delete_credential(self, key: str):
        """Delete stored credential."""
        try:
            keyring.delete_password(self.service_name, key)
            logger.info(f"ðŸ—‘ï¸ Credential '{key}' deleted")
        except keyring.errors.PasswordDeleteError:
            logger.warning(f"âš ï¸ Credential '{key}' not found")

class SecureSnowflakeConnection:
    """
    Snowflake connection using securely stored credentials.
    """
    
    def __init__(self, credential_manager: SecureCredentialManager):
        self.cred_mgr = credential_manager
        self.connection = None
    
    def connect(self):
        """Connect using securely stored credentials."""
        try:
            self.connection = snowflake.connector.connect(
                account=self.cred_mgr.retrieve_credential('account'),
                user=self.cred_mgr.retrieve_credential('user'),
                password=self.cred_mgr.retrieve_credential('password'),
                warehouse=self.cred_mgr.retrieve_credential('warehouse'),
                database=self.cred_mgr.retrieve_credential('database'),
                schema=self.cred_mgr.retrieve_credential('schema'),
                role=self.cred_mgr.retrieve_credential('role')
            )
            logger.info("âœ… Secure connection established")
            return self.connection
        except Exception as e:
            logger.error(f"âŒ Connection failed: {str(e)}")
            raise

# Usage example - First time setup
cred_mgr = SecureCredentialManager()

# Store credentials (do this once)
cred_mgr.store_credential('account', 'your_account.region')
cred_mgr.store_credential('user', 'your_username')
cred_mgr.store_credential('password', 'your_password')
cred_mgr.store_credential('warehouse', 'COMPUTE_WH')
cred_mgr.store_credential('database', 'your_database')
cred_mgr.store_credential('schema', 'your_schema')
cred_mgr.store_credential('role', 'your_role')

# Later usage - retrieve and connect
secure_conn = SecureSnowflakeConnection(cred_mgr)
connection = secure_conn.connect()
```

### SQL Injection Prevention

```python
class SQLInjectionPrevention:
    """
    Utilities for preventing SQL injection attacks.
    """
    
    @staticmethod
    def validate_identifier(identifier: str) -> bool:
        """
        Validate that identifier (table/column name) is safe.
        
        Args:
            identifier (str): Identifier to validate
            
        Returns:
            bool: True if valid
        """
        import re
        
        # Allow only alphanumeric characters and underscores
        pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'
        
        if not re.match(pattern, identifier):
            raise ValueError(
                f"Invalid identifier '{identifier}'. "
                "Only alphanumeric characters and underscores allowed."
            )
        
        return True
    
    @staticmethod
    def sanitize_string_value(value: str) -> str:
        """
        Sanitize string value to prevent SQL injection.
        
        Args:
            value (str): Value to sanitize
            
        Returns:
            str: Sanitized value
        """
        # Escape single quotes
        return value.replace("'", "''")
    
    @staticmethod
    def build_safe_query(
        table: str,
        columns: List[str],
        where_conditions: Dict[str, Any]
    ) -> tuple:
        """
        Build SQL query safely with parameterization.
        
        Args:
            table (str): Table name
            columns (list): Column names
            where_conditions (dict): WHERE clause conditions
            
        Returns:
            tuple: (query, parameters)
        """
        # Validate all identifiers
        SQLInjectionPrevention.validate_identifier(table)
        for col in columns:
            SQLInjectionPrevention.validate_identifier(col)
        
        for col in where_conditions.keys():
            SQLInjectionPrevention.validate_identifier(col)
        
        # Build query with parameters
        select_clause = ", ".join(columns)
        query = f"SELECT {select_clause} FROM {table}"
        
        params = {}
        if where_conditions:
            conditions = []
            for i, (col, value) in enumerate(where_conditions.items()):
                param_name = f"param_{i}"
                conditions.append(f"{col} = %({param_name})s")
                params[param_name] = value
            
            query += " WHERE " + " AND ".join(conditions)
        
        return query, params

# Usage example
sql_safe = SQLInjectionPrevention()

# Safe query building
table_name = "CUSTOMERS"
columns = ["customer_id", "customer_name", "email"]
filters = {"country": "USA", "status": "active"}

query, params = sql_safe.build_safe_query(table_name, columns, filters)

with SnowflakeConnection() as sf_conn:
    agent = SnowflakeAgent(sf_conn.connection)
    result = agent.execute_query(query, params)
    print(result)

# This will raise ValueError (preventing injection)
try:
    sql_safe.validate_identifier("users; DROP TABLE customers--")
except ValueError as e:
    print(f"âœ… SQL injection attempt blocked: {str(e)}")
```


---

## 11. Performance Optimization

### Query Performance Monitoring

```python
import time
from contextlib import contextmanager
from typing import Dict, List

class PerformanceMonitor:
    """
    Monitors and logs query performance metrics.
    """
    
    def __init__(self):
        self.metrics = []
    
    @contextmanager
    def measure_query(self, query_name: str):
        """
        Context manager to measure query execution time.
        
        Args:
            query_name (str): Identifier for the query
            
        Yields:
            dict: Metrics dictionary
        """
        start_time = time.time()
        metrics = {'query_name': query_name, 'start_time': start_time}
        
        try:
            yield metrics
            metrics['status'] = 'success'
        except Exception as e:
            metrics['status'] = 'failed'
            metrics['error'] = str(e)
            raise
        finally:
            end_time = time.time()
            metrics['end_time'] = end_time
            metrics['duration'] = end_time - start_time
            self.metrics.append(metrics)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Calculate performance statistics."""
        if not self.metrics:
            return {}
        
        durations = [m['duration'] for m in self.metrics if m['status'] == 'success']
        
        return {
            'total_queries': len(self.metrics),
            'successful_queries': sum(1 for m in self.metrics if m['status'] == 'success'),
            'failed_queries': sum(1 for m in self.metrics if m['status'] == 'failed'),
            'avg_duration': sum(durations) / len(durations) if durations else 0,
            'min_duration': min(durations) if durations else 0,
            'max_duration': max(durations) if durations else 0
        }
    
    def get_slow_queries(self, threshold_seconds: float = 5.0) -> List[Dict]:
        """
        Get queries that exceeded duration threshold.
        
        Args:
            threshold_seconds (float): Duration threshold
            
        Returns:
            list: Slow query metrics
        """
        return [
            m for m in self.metrics 
            if m.get('duration', 0) > threshold_seconds
        ]
    
    def export_metrics(self, filename: str):
        """Export metrics to JSON file."""
        import json
        
        with open(filename, 'w') as f:
            json.dump(self.metrics, f, indent=2, default=str)
        
        logger.info(f"ðŸ“Š Metrics exported to {filename}")

class OptimizedSnowflakeAgent(SnowflakeAgent):
    """
    Agent with performance monitoring and optimization.
    """
    
    def __init__(self, connection):
        super().__init__(connection)
        self.monitor = PerformanceMonitor()
    
    def execute_query(self, sql: str, params: Dict = None, 
                     query_name: str = None) -> pd.DataFrame:
        """
        Execute query with performance monitoring.
        
        Args:
            sql (str): SQL query
            params (dict): Query parameters
            query_name (str): Query identifier for monitoring
            
        Returns:
            pd.DataFrame: Query results
        """
        query_id = query_name or sql[:50]
        
        with self.monitor.measure_query(query_id) as metrics:
            result = super().execute_query(sql, params)
            metrics['rows_returned'] = len(result)
            metrics['columns'] = len(result.columns)
        
        return result
    
    def analyze_query_plan(self, sql: str) -> pd.DataFrame:
        """
        Get query execution plan for optimization analysis.
        
        Args:
            sql (str): SQL query to analyze
            
        Returns:
            pd.DataFrame: Execution plan details
        """
        explain_query = f"EXPLAIN {sql}"
        return super().execute_query(explain_query)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics."""
        return self.monitor.get_statistics()

# Usage example
with SnowflakeConnection() as sf_conn:
    agent = OptimizedSnowflakeAgent(sf_conn.connection)
    
    # Execute queries with monitoring
    queries = [
        ("SELECT * FROM CUSTOMERS WHERE country = 'USA'", "customer_query"),
        ("SELECT SUM(total_amount) FROM ORDERS", "revenue_query"),
        ("SELECT * FROM PRODUCTS JOIN ORDERS ON products.id = orders.product_id", "join_query")
    ]
    
    for sql, name in queries:
        try:
            result = agent.execute_query(sql, query_name=name)
            print(f"âœ… {name}: {len(result)} rows")
        except Exception as e:
            print(f"âŒ {name} failed: {str(e)}")
    
    # Get performance statistics
    stats = agent.get_performance_stats()
    print(f"\nPerformance Statistics:")
    print(f"  Total Queries: {stats['total_queries']}")
    print(f"  Average Duration: {stats['avg_duration']:.2f}s")
    print(f"  Max Duration: {stats['max_duration']:.2f}s")
    
    # Identify slow queries
    slow_queries = agent.monitor.get_slow_queries(threshold_seconds=2.0)
    if slow_queries:
        print(f"\nâš ï¸ Slow Queries (> 2s):")
        for query in slow_queries:
            print(f"  - {query['query_name']}: {query['duration']:.2f}s")
    
    # Export metrics
    agent.monitor.export_metrics('/tmp/query_metrics.json')
```

### Connection Pooling

```python
from queue import Queue, Empty
import threading

class SnowflakeConnectionPool:
    """
    Implements connection pooling for improved performance
    in multi-threaded applications.
    """
    
    def __init__(self, pool_size: int = 5, **connection_params):
        """
        Initialize connection pool.
        
        Args:
            pool_size (int): Number of connections to maintain
            **connection_params: Snowflake connection parameters
        """
        self.pool_size = pool_size
        self.connection_params = connection_params
        self.pool = Queue(maxsize=pool_size)
        self.lock = threading.Lock()
        self._initialize_pool()
    
    def _initialize_pool(self):
        """Create initial pool of connections."""
        for i in range(self.pool_size):
            try:
                conn = snowflake.connector.connect(**self.connection_params)
                self.pool.put(conn)
                logger.info(f"âœ… Connection {i+1}/{self.pool_size} added to pool")
            except Exception as e:
                logger.error(f"âŒ Failed to create connection {i+1}: {str(e)}")
    
    def get_connection(self, timeout: int = 30):
        """
        Get connection from pool.
        
        Args:
            timeout (int): Timeout in seconds
            
        Returns:
            snowflake.connector.connection: Database connection
        """
        try:
            conn = self.pool.get(timeout=timeout)
            
            # Test if connection is still alive
            try:
                conn.cursor().execute("SELECT 1")
                return conn
            except Exception:
                # Connection is dead, create new one
                logger.warning("âš ï¸ Stale connection detected, creating new one")
                conn = snowflake.connector.connect(**self.connection_params)
                return conn
                
        except Empty:
            raise TimeoutError(f"No connection available within {timeout}s")
    
    def return_connection(self, conn):
        """
        Return connection to pool.
        
        Args:
            conn: Database connection to return
        """
        try:
            self.pool.put(conn, block=False)
        except:
            # Pool is full, close the connection
            conn.close()
    
    def close_all(self):
        """Close all connections in pool."""
        while not self.pool.empty():
            try:
                conn = self.pool.get(block=False)
                conn.close()
            except Empty:
                break
        logger.info("ðŸ”’ All pool connections closed")
    
    @contextmanager
    def get_connection_context(self):
        """Context manager for getting/returning connections."""
        conn = self.get_connection()
        try:
            yield conn
        finally:
            self.return_connection(conn)

# Usage example
connection_params = {
    'account': os.getenv('SNOWFLAKE_ACCOUNT'),
    'user': os.getenv('SNOWFLAKE_USER'),
    'password': os.getenv('SNOWFLAKE_PASSWORD'),
    'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE'),
    'database': os.getenv('SNOWFLAKE_DATABASE'),
    'schema': os.getenv('SNOWFLAKE_SCHEMA'),
    'role': os.getenv('SNOWFLAKE_ROLE')
}

# Create connection pool
pool = SnowflakeConnectionPool(pool_size=5, **connection_params)

# Use connections from pool
def execute_query_from_pool(query: str):
    with pool.get_connection_context() as conn:
        cursor = conn.cursor()
        cursor.execute(query)
        result = cursor.fetchall()
        return result

# Multi-threaded execution
import concurrent.futures

queries = [
    "SELECT COUNT(*) FROM ORDERS",
    "SELECT COUNT(*) FROM CUSTOMERS",
    "SELECT COUNT(*) FROM PRODUCTS"
]

with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    results = list(executor.map(execute_query_from_pool, queries))
    print(f"Results: {results}")

# Cleanup
pool.close_all()
```

---

## 12. Complete Example Application

### End-to-End Snowflake Analytics Agent

```python
#!/usr/bin/env python3
"""
Complete Snowflake Analytics Agent
Demonstrates all concepts covered in this guide.
"""

import os
import sys
from datetime import datetime
from typing import Optional
import pandas as pd

class SnowflakeAnalyticsAgent:
    """
    Complete analytics agent combining all features:
    - AI-driven query generation
    - Result caching
    - Performance monitoring
    - Data visualization
    - Error handling
    """
    
    def __init__(self, use_cache: bool = True, cache_ttl: int = 3600):
        """
        Initialize the complete analytics agent.
        
        Args:
            use_cache (bool): Enable result caching
            cache_ttl (int): Cache time-to-live in seconds
        """
        # Load environment variables
        load_dotenv()
        
        # Initialize components
        self.connection = None
        self.agent = None
        self.ai_generator = None
        self.cache = None
        self.monitor = PerformanceMonitor()
        self.use_cache = use_cache
        
        logger.info("ðŸš€ Initializing Snowflake Analytics Agent")
        
        # Setup connection
        self._setup_connection()
        
        # Setup AI query generator
        self._setup_ai_generator()
        
        # Setup caching
        if use_cache:
            self._setup_cache(cache_ttl)
        
        logger.info("âœ… Agent initialization complete")
    
    def _setup_connection(self):
        """Setup Snowflake connection."""
        try:
            sf_conn = SnowflakeConnection()
            self.connection = sf_conn.connect()
            self.agent = OptimizedSnowflakeAgent(self.connection)
            logger.info("âœ… Snowflake connection established")
        except Exception as e:
            logger.error(f"âŒ Connection failed: {str(e)}")
            sys.exit(1)
    
    def _setup_ai_generator(self):
        """Setup AI query generator with schema context."""
        try:
            # Get schema context
            discovery = SchemaDiscovery(self.connection)
            schema_context = discovery.generate_schema_context()
            
            # Initialize AI generator
            self.ai_generator = AIQueryGenerator(schema_context)
            logger.info("âœ… AI query generator initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ AI generator setup failed: {str(e)}")
            self.ai_generator = None
    
    def _setup_cache(self, ttl: int):
        """Setup query result cache."""
        try:
            self.cache = QueryCache(max_size=100, ttl_minutes=ttl // 60)
            logger.info(f"âœ… Cache initialized (TTL: {ttl}s)")
        except Exception as e:
            logger.warning(f"âš ï¸ Cache setup failed: {str(e)}")
            self.cache = None
    
    def ask(self, question: str) -> pd.DataFrame:
        """
        Ask a question in natural language and get results.
        
        Args:
            question (str): Natural language question
            
        Returns:
            pd.DataFrame: Query results
        """
        logger.info(f"ðŸ“ Question: {question}")
        
        if not self.ai_generator:
            raise RuntimeError("AI query generator not available")
        
        # Generate SQL from natural language
        with self.monitor.measure_query('ai_query_generation'):
            sql = self.ai_generator.generate_sql(question)
            logger.info(f"ðŸ¤– Generated SQL:\n{sql}")
        
        # Execute query
        result = self.execute(sql)
        
        # Get explanation
        explanation = self.ai_generator.explain_query(sql)
        logger.info(f"ðŸ’¡ Explanation: {explanation}")
        
        return result
    
    def execute(self, sql: str, params: Dict = None) -> pd.DataFrame:
        """
        Execute SQL query with caching and monitoring.
        
        Args:
            sql (str): SQL query
            params (dict): Query parameters
            
        Returns:
            pd.DataFrame: Query results
        """
        # Check cache
        if self.use_cache and self.cache:
            cached_result = self.cache.get(sql, params)
            if cached_result is not None:
                return cached_result
        
        # Execute query with monitoring
        with self.monitor.measure_query('sql_execution'):
            result = self.agent.execute_query(sql, params)
        
        # Store in cache
        if self.use_cache and self.cache:
            self.cache.set(sql, result, params)
        
        return result
    
    def visualize(self, df: pd.DataFrame, chart_type: str = 'bar',
                  title: str = None, save_path: str = None):
        """
        Create visualization from query results.
        
        Args:
            df (pd.DataFrame): Data to visualize
            chart_type (str): Type of chart ('bar', 'line', 'pie')
            title (str): Chart title
            save_path (str): Path to save chart
        """
        viz = DataVisualizer()
        
        if chart_type == 'bar' and len(df.columns) >= 2:
            viz.plot_bar_chart(
                df, 
                x_col=df.columns[0],
                y_col=df.columns[1],
                title=title,
                save_path=save_path
            )
        elif chart_type == 'line' and len(df.columns) >= 2:
            viz.plot_line_chart(
                df,
                x_col=df.columns[0],
                y_cols=list(df.columns[1:]),
                title=title,
                save_path=save_path
            )
        else:
            logger.warning(f"âš ï¸ Unsupported chart type or insufficient columns")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report."""
        stats = self.monitor.get_statistics()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'query_statistics': stats,
            'slow_queries': self.monitor.get_slow_queries(),
            'cache_stats': self.cache.get_stats() if self.cache else None
        }
        
        return report
    
    def close(self):
        """Cleanup and close connections."""
        if self.connection:
            self.connection.close()
            logger.info("ðŸ”’ Connection closed")

def main():
    """Main application demonstrating agent usage."""
    
    # Initialize agent
    agent = SnowflakeAnalyticsAgent(use_cache=True)
    
    try:
        # Example 1: Ask natural language question
        print("\n" + "="*60)
        print("Example 1: Natural Language Query")
        print("="*60)
        
        question = "What are the top 10 customers by total order value?"
        results = agent.ask(question)
        print(results)
        
        # Example 2: Direct SQL execution
        print("\n" + "="*60)
        print("Example 2: Direct SQL Query")
        print("="*60)
        
        sql = """
        SELECT 
            DATE_TRUNC('month', order_date) as month,
            SUM(total_amount) as revenue
        FROM ORDERS
        WHERE order_date >= '2024-01-01'
        GROUP BY month
        ORDER BY month
        """
        
        results = agent.execute(sql)
        print(results)
        
        # Example 3: Visualize results
        print("\n" + "="*60)
        print("Example 3: Data Visualization")
        print("="*60)
        
        agent.visualize(
            results,
            chart_type='line',
            title='Monthly Revenue Trend 2024',
            save_path='/tmp/revenue_trend.png'
        )
        
        # Example 4: Performance report
        print("\n" + "="*60)
        print("Example 4: Performance Report")
        print("="*60)
        
        report = agent.get_performance_report()
        print(f"Total Queries: {report['query_statistics']['total_queries']}")
        print(f"Average Duration: {report['query_statistics']['avg_duration']:.2f}s")
        print(f"Cache Hits: {report['cache_stats']['size'] if report['cache_stats'] else 'N/A'}")
        
    except Exception as e:
        logger.error(f"âŒ Application error: {str(e)}")
        raise
    finally:
        agent.close()

if __name__ == "__main__":
    main()
```

### Command-Line Interface

```python
#!/usr/bin/env python3
"""
CLI for Snowflake Analytics Agent
"""

import argparse
import json

def cli_main():
    """Command-line interface entry point."""
    
    parser = argparse.ArgumentParser(
        description='Snowflake Analytics Agent CLI'
    )
    
    parser.add_argument(
        'command',
        choices=['ask', 'execute', 'schema', 'stats'],
        help='Command to execute'
    )
    
    parser.add_argument(
        '--query',
        '-q',
        type=str,
        help='SQL query or natural language question'
    )
    
    parser.add_argument(
        '--output',
        '-o',
        type=str,
        help='Output file path (CSV or JSON)'
    )
    
    parser.add_argument(
        '--format',
        '-f',
        choices=['csv', 'json', 'table'],
        default='table',
        help='Output format'
    )
    
    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='Disable query caching'
    )
    
    args = parser.parse_args()
    
    # Initialize agent
    agent = SnowflakeAnalyticsAgent(use_cache=not args.no_cache)
    
    try:
        if args.command == 'ask':
            if not args.query:
                print("Error: --query required for 'ask' command")
                return
            
            result = agent.ask(args.query)
            output_result(result, args.format, args.output)
        
        elif args.command == 'execute':
            if not args.query:
                print("Error: --query required for 'execute' command")
                return
            
            result = agent.execute(args.query)
            output_result(result, args.format, args.output)
        
        elif args.command == 'schema':
            discovery = SchemaDiscovery(agent.connection)
            tables = discovery.get_all_tables()
            output_result(tables, args.format, args.output)
        
        elif args.command == 'stats':
            report = agent.get_performance_report()
            print(json.dumps(report, indent=2, default=str))
    
    finally:
        agent.close()

def output_result(df: pd.DataFrame, format: str, output_path: Optional[str]):
    """Output DataFrame in specified format."""
    
    if format == 'csv':
        if output_path:
            df.to_csv(output_path, index=False)
            print(f"âœ… Results saved to {output_path}")
        else:
            print(df.to_csv(index=False))
    
    elif format == 'json':
        if output_path:
            df.to_json(output_path, orient='records', indent=2)
            print(f"âœ… Results saved to {output_path}")
        else:
            print(df.to_json(orient='records', indent=2))
    
    else:  # table format
        print(df.to_string(index=False))

if __name__ == "__main__":
    cli_main()
```

---

## 13. Resources & References

### Official Documentation

- **Snowflake Documentation:** [docs.snowflake.com](https://docs.snowflake.com)
- **Snowflake Python Connector:** [docs.snowflake.com/en/user-guide/python-connector](https://docs.snowflake.com/en/user-guide/python-connector.html)
- **SQLAlchemy Snowflake Dialect:** [docs.snowflake.com/en/user-guide/sqlalchemy](https://docs.snowflake.com/en/user-guide/sqlalchemy.html)

### AI & LangChain Resources

- **OpenAI API Documentation:** [platform.openai.com/docs](https://platform.openai.com/docs)
- **LangChain Documentation:** [python.langchain.com](https://python.langchain.com)
- **LangChain SQL Agent:** [python.langchain.com/docs/use_cases/sql](https://python.langchain.com/docs/use_cases/sql)

### Python Libraries

- **Pandas Documentation:** [pandas.pydata.org](https://pandas.pydata.org)
- **Matplotlib Documentation:** [matplotlib.org](https://matplotlib.org)
- **Plotly Documentation:** [plotly.com/python](https://plotly.com/python/)
- **Redis Python Client:** [redis-py.readthedocs.io](https://redis-py.readthedocs.io)

### Best Practices & Tutorials

- **Snowflake Best Practices:** [docs.snowflake.com/en/user-guide/ui-snowsight-best-practices](https://docs.snowflake.com/en/user-guide/ui-snowsight-best-practices.html)
- **Query Optimization Guide:** [docs.snowflake.com/en/user-guide/ui-query-profile](https://docs.snowflake.com/en/user-guide/ui-query-profile.html)
- **Security Best Practices:** [docs.snowflake.com/en/user-guide/security](https://docs.snowflake.com/en/user-guide/security.html)

### Community & Support

- **Snowflake Community:** [community.snowflake.com](https://community.snowflake.com)
- **Stack Overflow:** [stackoverflow.com/questions/tagged/snowflake-cloud-data-platform](https://stackoverflow.com/questions/tagged/snowflake-cloud-data-platform)
- **GitHub Examples:** Search for "snowflake-connector-python" repositories

### Additional Tools

- **DBeaver:** Universal database tool with Snowflake support
- **SnowSQL:** Command-line client for Snowflake
- **Snowflake Web Interface:** Built-in query editor and monitoring

---

## Conclusion

This comprehensive guide has covered everything you need to create sophisticated agents that interact with Snowflake data warehouses. From basic connection setup to advanced AI-driven query generation, caching strategies, and data visualization, you now have a complete toolkit for building production-ready Snowflake analytics applications.

### Key Takeaways

1. **Security First:** Always use secure credential management and parameterized queries
2. **Performance Matters:** Implement caching and connection pooling for optimal performance
3. **AI Enhancement:** Leverage AI models for natural language query generation
4. **Monitor Everything:** Track query performance and optimize based on metrics
5. **Error Handling:** Implement robust retry logic and error recovery
6. **Visualization:** Transform data into actionable insights with charts and dashboards

### Next Steps

- Experiment with the provided code examples
- Customize agents for your specific use cases
- Explore advanced Snowflake features (streams, tasks, stored procedures)
- Integrate with your existing data pipeline and BI tools
- Continuously monitor and optimize performance

Happy coding! ðŸš€

---

*Last Updated: January 2026*
*Version: 1.0*
